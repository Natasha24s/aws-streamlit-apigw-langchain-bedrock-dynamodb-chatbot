AWSTemplateFormatVersion: '2010-09-09'
Description: 'Chatbot Infrastructure with AWS Bedrock, API Gateway, Lambda, DynamoDB, and Guardrail'

Parameters:
  BucketName:
    Type: String
    Description: Name for the S3 bucket (must be globally unique)

Resources:
  # S3 Bucket
  ChatbotBucket:
    Type: AWS::S3::Bucket
    Properties:
      BucketName: !Ref BucketName
      VersioningConfiguration:
        Status: Enabled
      BucketEncryption:
        ServerSideEncryptionConfiguration:
          - ServerSideEncryptionByDefault:
              SSEAlgorithm: AES256

  # IAM Role for Lambda
  LambdaExecutionRole:
    Type: AWS::IAM::Role
    Properties:
      AssumeRolePolicyDocument:
        Version: '2012-10-17'
        Statement:
          - Effect: Allow
            Principal:
              Service: lambda.amazonaws.com
            Action: sts:AssumeRole
      ManagedPolicyArns:
        - arn:aws:iam::aws:policy/service-role/AWSLambdaBasicExecutionRole
      Policies:
        - PolicyName: BedrockAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:*
                Resource: '*'
        - PolicyName: DynamoDBAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - dynamodb:GetItem
                  - dynamodb:PutItem
                Resource: !GetAtt ConversationHistoryTable.Arn
        - PolicyName: KnowledgeBasesAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - kendra:Retrieve
                Resource: '*'
        - PolicyName: BedrockGuardrailAccess
          PolicyDocument:
            Version: '2012-10-17'
            Statement:
              - Effect: Allow
                Action:
                  - bedrock:EvaluateGuardrail
                Resource: !GetAtt ProductAvailabilityGuardrail.Arn

  # DynamoDB Table
  ConversationHistoryTable:
    Type: AWS::DynamoDB::Table
    Properties:
      TableName: ConversationHistory
      BillingMode: PAY_PER_REQUEST
      AttributeDefinitions:
        - AttributeName: SessionId
          AttributeType: S
      KeySchema:
        - AttributeName: SessionId
          KeyType: HASH

  # API Gateway
  ChatbotApi:
    Type: AWS::ApiGateway::RestApi
    Properties:
      Name: ChatbotAPI
      Description: API for Chatbot

  ApiResource:
    Type: AWS::ApiGateway::Resource
    Properties:
      ParentId: !GetAtt ChatbotApi.RootResourceId
      PathPart: chat
      RestApiId: !Ref ChatbotApi

  ApiMethod:
    Type: AWS::ApiGateway::Method
    Properties:
      HttpMethod: POST
      ResourceId: !Ref ApiResource
      RestApiId: !Ref ChatbotApi
      AuthorizationType: NONE
      Integration:
        Type: AWS
        IntegrationHttpMethod: POST
        Uri: !Sub arn:aws:apigateway:${AWS::Region}:lambda:path/2015-03-31/functions/${ChatbotLambda.Arn}/invocations
        IntegrationResponses:
          - StatusCode: 200
            SelectionPattern: ''
            ResponseTemplates:
              application/json: ''
        PassthroughBehavior: WHEN_NO_TEMPLATES
      MethodResponses:
        - StatusCode: '200'
          ResponseModels:
            application/json: 'Empty'

  ApiDeployment:
    Type: AWS::ApiGateway::Deployment
    DependsOn: ApiMethod
    Properties:
      RestApiId: !Ref ChatbotApi

  ApiStage:
    Type: AWS::ApiGateway::Stage
    Properties:
      DeploymentId: !Ref ApiDeployment
      RestApiId: !Ref ChatbotApi
      StageName: prod

  # Guardrail for Product Availability
  ProductAvailabilityGuardrail:
    Type: AWS::Bedrock::Guardrail
    Properties:
      BlockedInputMessaging: "I'm sorry, but I can't process queries about specific product availability."
      BlockedOutputsMessaging: "I apologize, but I can't provide specific information about product availability."
      ContentPolicyConfig:
        FiltersConfig:
          - InputStrength: "MEDIUM"
            OutputStrength: "HIGH"
            Type: "VIOLENCE"
      Description: "Guardrail designed to block detailed queries and responses about product availability"
      Name: "ProductAvailabilityGuardrail"
      TopicPolicyConfig:
        TopicsConfig:
          - Definition: "Specific information about the current stock or availability of products"
            Examples:
              - "Is this product in stock?"
              - "When will you have more of this item?"
              - "How many units of this product are available?"
            Name: "ProductAvailability"
            Type: "DENY"

  # Lambda Function
  ChatbotLambda:
    Type: AWS::Lambda::Function
    Properties:
      Handler: index.lambda_handler
      Role: !GetAtt LambdaExecutionRole.Arn
      Code:
        ZipFile: |
          import json
          import os
          import boto3
          import time
          from typing import Any, List, Mapping, Optional, Iterator
          from langchain.callbacks.manager import CallbackManagerForLLMRun
          from langchain.prompts import PromptTemplate
          import logging
          from langchain.schema import AIMessage, BaseMessage, ChatResult, HumanMessage, SystemMessage, ChatGeneration
          from langchain_community.retrievers import AmazonKnowledgeBasesRetriever
          from langchain.chat_models.base import BaseChatModel
          from pydantic import Field
          from langchain_community.chat_message_histories import DynamoDBChatMessageHistory
          import uuid

          session_id = str(uuid.uuid4())

          # Set up logging
          logger = logging.getLogger()
          logger.setLevel(logging.INFO)

          # Initialize AWS clients
          bedrock_runtime = boto3.client("bedrock-runtime")
          bedrock = boto3.client('bedrock')
          dynamodb = boto3.client('dynamodb')

          class BedrockLlama3ChatModel(BaseChatModel):
              model_id: str = Field(..., description="The Bedrock model ID")
              client: Any = Field(..., description="The Bedrock client")
              max_tokens: int = Field(512, description="Maximum number of tokens to generate")
              temperature: float = Field(0.7, description="Temperature for response generation")
              top_p: float = Field(0.9, description="Top p for response generation")

              class Config:
                  arbitrary_types_allowed = True

              @property
              def _llm_type(self) -> str:
                  return "bedrock-llama3-chat"

              def _generate(
                  self,
                  messages: List[BaseMessage],
                  stop: Optional[List[str]] = None,
                  run_manager: Optional[CallbackManagerForLLMRun] = None,
                  **kwargs: Any,
              ) -> Iterator[str]:
                  prompt = self._convert_messages_to_prompt(messages)

                  body = json.dumps({
                      "prompt": prompt,
                      "max_gen_len": self.max_tokens,
                      "temperature": self.temperature,
                      "top_p": self.top_p
                  })

                  response_stream = self.client.invoke_model_with_response_stream(
                      modelId=self.model_id,
                      contentType="application/json",
                      accept="application/json",
                      body=body
                  )

                  return self._process_stream(response_stream)

              def _process_stream(self, response_stream: Iterator[Any]) -> Iterator[str]:
                  for event in response_stream['body']:
                      if 'chunk' in event:
                          chunk = json.loads(event['chunk']['bytes'].decode())
                          if 'generation' in chunk:
                              yield chunk['generation']

              def _convert_messages_to_prompt(self, messages: List[BaseMessage]) -> str:
                  prompt = ""
                  for message in messages:
                      if isinstance(message, SystemMessage):
                          prompt += f"[INST] <<SYS>>\n{message.content}\n<</SYS>>\n\n"
                      elif isinstance(message, HumanMessage):
                          prompt += f"{message.content} [/INST]\n"
                      elif isinstance(message, AIMessage):
                          prompt += f"{message.content}\n\n"
                  return prompt

              @property
              def _identifying_params(self) -> Mapping[str, Any]:
                  return {"model_id": self.model_id}

          def lambda_handler(event, context):
              logger.info(f"Received event: {json.dumps(event, indent=2)}")

              # Get environment variables
              bedrock_model_id = os.environ['BEDROCK_MODEL_ID']
              knowledge_base_id = os.environ['KNOWLEDGE_BASE_ID']
              guardrail_id = os.environ['GUARDRAIL_ID']
              region_name = boto3.session.Session().region_name

              logger.info(f"Using Bedrock Model ID: {bedrock_model_id}")
              logger.info(f"Using Knowledge Base ID: {knowledge_base_id}")
              logger.info(f"Using Guardrail ID: {guardrail_id}")
              logger.info(f"Using Region: {region_name}")

              # Define system message
              system_message = """
              You are an AI assistant for product information. Be concise in your responses. Always be polite and professional.
              Never provide information about competitors' products.
              Do not discuss availability. If asked, say this information changes frequently and encourage users to visit our website or contact customer service for the most up-to-date information.
              Always respect user privacy and do not ask for or store personal information.
              """

              # Initialize BedrockLlama3ChatModel
              llm = BedrockLlama3ChatModel(
                  model_id=bedrock_model_id, 
                  client=bedrock_runtime,
                  max_tokens=512,
                  temperature=0.7,
                  top_p=0.9
              )

              try:
                  # Get user input from the event
                  user_input = event.get('query')

                  logger.info(f"User input: {user_input}")
                  logger.info(f"Session ID: {session_id}")

                  if not user_input:
                      logger.error("No query provided in the event")
                      return {
                          'statusCode': 400,
                          'body': json.dumps({'error': 'No query provided in the event'})
                      }

                  # Apply guardrail to user input
                  guardrail_response = bedrock.evaluate_guardrail(
                      guardrailId=guardrail_id,
                      inputText=user_input
                  )

                  if guardrail_response['results'][0]['blocklistMatched']:
                      return {
                          'statusCode': 200,
                          'body': json.dumps({
                              'query': user_input,
                              'generated_response': guardrail_response['results'][0]['outputText']
                          })
                      }

                  # Initialize DynamoDBChatMessageHistory
                  message_history = DynamoDBChatMessageHistory(
                      table_name="ConversationHistory",
                      session_id=session_id,
                  )

                  # Retrieve chat history
                  chat_history = message_history.messages

                  # Add the new user message to the history
                  message_history.add_user_message(user_input)

                  # Initialize AmazonKnowledgeBasesRetriever
                  retriever = AmazonKnowledgeBasesRetriever(
                      knowledge_base_id=knowledge_base_id,
                      retrieval_config={"vectorSearchConfiguration": {"numberOfResults": 4}},
                      region_name=region_name,
                      return_source_documents=True
                  )

                  # Retrieve relevant documents
                  docs = retriever.get_relevant_documents(user_input)
                  context = "\n".join([f"Source {i+1}: {doc.page_content}" for i, doc in enumerate(docs)])

                  # Create a custom prompt template
                  prompt_template = """
                  Use the following context and chat history to answer the question. Your response should be a concise summary of the information found in the search results, with key points or features listed in bullet points if appropriate.

                  Context:
                  {context}

                  Chat History:
                  {chat_history}

                  Question: {question}

                  Generate a response in the following format:
                  Based on the search results, here's the information about [topic of the question]:
                  - [Key point or feature 1]
                  - [Key point or feature 2]
                  - [Key point or feature 3]
                  [Additional information if necessary]

                  Answer:
                  """

                  prompt = PromptTemplate(template=prompt_template, input_variables=["context", "chat_history", "question"])

                  # Generate response
                  messages = [
                      SystemMessage(content=system_message),
                      HumanMessage(content=prompt.format(
                          context=context, 
                          chat_history="\n".join([f"{m.type}: {m.content}" for m in chat_history]), 
                          question=user_input
                      ))
                  ]

                  # Generate the full response
                  full_response = "".join(llm._generate(messages))

                  # Add the AI's response to the chat history
                  message_history.add_ai_message(full_response)

                  return {
                      'statusCode': 200,
                      'body': json.dumps({
                          'query': user_input,
                          'generated_response': full_response
                      })
                  }

              except Exception as e:
                  logger.error(f"An error occurred: {str(e)}", exc_info=True)
                  return {
                      'statusCode': 500,
                      'body': json.dumps({
                          'error': 'An unexpected error occurred.',
                          'details': str(e)
                      })
                  }
      Runtime: python3.9
      Timeout: 300
      MemorySize: 256
      Environment:
        Variables:
          KNOWLEDGE_BASE_ID: 'dummy-knowledge-base-id'
          BEDROCK_MODEL_ID: 'dummy-bedrock-model-id'
          GUARDRAIL_ID: !Ref ProductAvailabilityGuardrail

  # Lambda Permission for API Gateway
  LambdaPermission:
    Type: AWS::Lambda::Permission
    Properties:
      Action: lambda:InvokeFunction
      FunctionName: !Ref ChatbotLambda
      Principal: apigateway.amazonaws.com
      SourceArn: !Sub arn:aws:execute-api:${AWS::Region}:${AWS::AccountId}:${ChatbotApi}/*/POST/chat

Outputs:
  ApiEndpoint:
    Description: API Endpoint URL
    Value: !Sub https://${ChatbotApi}.execute-api.${AWS::Region}.amazonaws.com/prod/chat

  DynamoDBTableName:
    Description: DynamoDB Table Name
    Value: !Ref ConversationHistoryTable

  S3BucketName:
    Description: S3 Bucket Name
    Value: !Ref ChatbotBucket

  GuardrailId: 
    Description: Guardrail ID
    Value: !Ref ProductAvailabilityGuardrail

